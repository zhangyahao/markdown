####配置文件优化   
1.  内存锁定   
    bootstrap.memory_lock：true 允许 JVM 锁住内存，禁止操作系统交换出去。
2.  zen.discovery  
    Elasticsearch 默认被配置为使用单播发现，以防止节点无意中加入集群。组播发现应该永远不被使用在生产环境了，否则你得到的结果就是一个节点
    意外的加入到了你的生产环境，仅仅是因为他们收到了一个错误的组播信号。ES是一个P2P类型的分布式系统，使用gossip协议，集群的任意请求都可以
    发送到集群的任一节点，然后es内部会找到需要转发的节点，并且与之进行通信。在es1.x的版本，es默认是开启组播，启动es之后，可以快速将局域网
    内集群名称，默认端口的相同实例加入到一个大的集群，后续再es2.x之后，都调整成了单播，避免安全问题和网络风暴；单播
    discovery.zen.ping.unicast.hosts，建议写入集群内所有的节点及端口，如果新实例加入集群，新实例只需要写入当前集群的实例，即可自动加
    入到当前集群，之后再处理原实例的配置即可，新实例加入集群，不需要重启原有实例；节点zen相关配置：discovery.zen.ping_timeout：
    判断master选举过程中，发现其他node存活的超时设置，主要影响选举的耗时，参数仅在加入或者选举 master 主节点的时候才起作用 
    discovery.zen.join_timeout：节点确定加入到集群中，向主节点发送加入请求的超时时间，默认为3s 
    discovery.zen.minimum_master_nodes：参与master选举的最小节点数，当集群能够被选为master的节点数量小于最小数量时，
    集群将无法正常选举。
3.  故障检测（ fault detection ）  
    两种情况下回进行故障检测，第一种是由master向集群的所有其他节点发起ping，验证节点是否处于活动状态；第二种是：集群每个节点
    向master发起ping，判断master是否存活，是否需要发起选举。故障检测需要配置以下设置使用 形如：
    discovery.zen.fd.ping_interval 节点被ping的频率，默认为1s。discovery.zen.fd.ping_timeout 等待ping响应的时间，
    默认为 30s，运行的集群中，master 检测所有节点，以及节点检测 master 是否正常。discovery.zen.fd.ping_retries ping
    失败/超时多少导致节点被视为失败，默认为3。  
    [官网解释](https://www.elastic.co/guide/en/elasticsearch/reference/6.x/modules-discovery-zen.html)
4.  队列数量  
    不建议盲目加大es的队列数量，如果是偶发的因为数据突增，导致队列阻塞，加大队列size可以使用内存来缓存数据，如果是持续性的数据阻塞在队列，
    加大队列size除了加大内存占用，并不能有效提高数据写入速率，反而可能加大es宕机时候，在内存中可能丢失的上数据量。
    哪些情况下，加大队列size呢？GET /_cat/thread_pool，观察api中返回的queue和rejected，如果确实存在队列拒绝或者是持续的queue
    ，可以酌情调整队列size。  
    [线程池中文文档](https://doc.codingdict.com/elasticsearch/437/)
5.  内存使用  
    设置indices的内存熔断相关参数，根据实际情况进行调整，防止写入或查询压力过高导致OOM， indices.breaker.total.limit: 50%，
    集群级别的断路器，默认为jvm堆的70%；indices.breaker.request.limit: 10%，单个request的断路器限制，默认为jvm堆的60%；
    indices.breaker.fielddata.limit: 10%，fielddata breaker限制，默认为jvm堆的60%。  
    [熔断器](https://doc.codingdict.com/elasticsearch/421/)
    根据实际情况调整查询占用cache，避免查询cache占用过多的jvm内存，参数为静态的，需要在每个数据节点配置。
    indices.queries.cache.size: 5%，控制过滤器缓存的内存大小，默认为10%。接受百分比值，5%或者精确值，例如512mb。
6.  创建shard  
    如果集群规模较大，可以阻止新建shard时扫描集群内全部shard的元数据，提升shard分配速度。
    cluster.routing.allocation.disk.include_relocations: false，默认为true。
    [Disk-based Shard Allocation ( 基于磁盘的分片分配 )](https://doc.codingdict.com/elasticsearch/443/)
####系统层面调优    
1.  jdk版本  
    当前根据官方建议，选择匹配的jdk版本；
2.  jdk内存配置   
    设置略小于机器内存的一半，剩余留给系统使用。同时，jvm heap建议不要超过32G（不同jdk版本具体的值会略有不同），
    否则jvm会因为内存指针压缩导致内存浪费，详见：
    [在jvm.options中设置JVM堆大小](https://doc.codingdict.com/elasticsearch/44/)
3.  交换分区  
    关闭交换分区，防止内存发生交换导致性能下降（部分情况下，宁死勿慢） swapoff -a
    [禁用swapping](https://doc.codingdict.com/elasticsearch/45/)
4.  文件句柄  
    **此项是必须得**  
    Lucene 使用了 大量的 文件。同时，Elasticsearch 在节点和 HTTP 客户端之间进行通信也使用了大量的套接字，所有这一切都需要足够的
    文件描述符，默认情况下，linux默认运行单个进程打开1024个文件句柄，这显然是不够的，故需要加大文件句柄数 ulimit -n 65536
5.  mmap  
    Elasticsearch 对各种文件混合使用了 NioFs（ 注：非阻塞文件系统）和 MMapFs （ 注：内存映射文件系统）。请确保你配置的最大映射数量，
    以便有足够的虚拟内存可用于 mmapped 文件。这可以暂时设置：sysctl -w vm.max_map_count=262144 或者你可以在 
    /etc/sysctl.conf 通过修改 vm.max_map_count 永久设置它。
6.  磁盘  
    如果你正在使用 SSDs，确保你的系统 I/O 调度程序是配置正确的。当你向硬盘写数据，I/O 调度程序决定何时把数据实际发送到硬盘。大多数默认
    nix 发行版下的调度程序都叫做 cfq（完全公平队列）。但它是为旋转介质优化的：机械硬盘的固有特性意味着它写入数据到基于物理布局的硬盘会更高效。
    这对 SSD 来说是低效的，尽管这里没有涉及到机械硬盘。但是，deadline 或者 noop 应该被使用。deadline 调度程序基于写入等待时间进行优化，
    noop 只是一个简单的 FIFO 队列。echo noop > /sys/block/sd/queue/scheduler
7.  磁盘挂载  
    mount -o noatime,data=writeback,barrier=0,nobh /dev/sd* /esdata* 其中，noatime，禁止记录访问时间戳；data=writeback，
    不记录journal；barrier=0，因为关闭了journal，所以同步关闭barrier；nobh，关闭buffer_head，防止内核影响数据IO
8.  磁盘其他注意事项  
    使用 RAID 0。条带化 RAID 会提高磁盘I/O，代价显然就是当一块硬盘故障时整个就故障了，不要使用镜像或者奇偶校验 RAID 因为副本已经提供了
    这个功能。另外，使用多块硬盘，并允许 Elasticsearch 通过多个 path.data 目录配置把数据条带化分配到它们上面。不要使用远程挂载的存储，
    比如 NFS 或者 SMB/CIFS。这个引入的延迟对性能来说完全是背道而驰的。